\documentclass[10pt]{article}
\pdfoutput=1
%\usepackage{NotesTeX,lipsum}
\usepackage{NotesTeX,lipsum}
%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Notes}}\\{{\itshape Quantum Mechanics}}\end{center}}
\author{Yi Huang\footnote{\href{https://yiihuang.com/}{\textit{My Personal Website}}}}


\affiliation{
University of Minnesota
}

\emailAdd{yihphysics@gmail.com}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
	\pagestyle{fancynotes}
	\part{Caprice}
	\section{Fall 2018}\label{sec:fall2018}
	\begin{margintable}\vspace{.8in}\footnotesize
		\begin{tabularx}{\marginparwidth}{|X}
		Section~\ref{sec:fall2018}. Fall 2018\\
		\end{tabularx}
	\end{margintable}

	This section is based on my Quantum mechanics course in Fall 2018. The textbook we use is Sakurai's \textit{Modern Quantum Mechanics}.

	\subsection{Derivation of (2.1.33)}


	Use iteration to prove it
	\begin{align*}
		i \hbar \frac{\partial}{\partial t} \mathcal{U}(t,t_0) &= H(t) \mathcal{U}(t,t_0) \\
		\mathcal{U}(t,t_0)
		&= \mathbb{I} + \int_{t_0}^{t} dt_1 \frac{H(t_1)}{i\hbar} \mathcal{U}(t_1,t_0) \\
		&= \mathbb{I} + \int_{t_0}^{t} dt_1 \frac{H(t_1)}{i\hbar} \left(\mathbb{I} +\int_{t_0}^{t_1} dt_2 \frac{H(t_2)}{i\hbar} \mathcal{U}(t_2,t_0)\right) \\
		&\vdots \\
		&= \mathbb{I} + \sum_{n=1}^{\infty}\left(\frac{1}{i \hbar}\right)^n \int_{t_0}^{t} dt_1 \int_{t_0}^{t_1} dt_2 \dots \int_{t_0}^{t_{n-1}} dt_{n} H(t_1)H(t_2) \dots H(t_n)
	\end{align*}

	\subsection{Postulate of quantum mechanics: inner product}

	The inner product postulate of quantum mechanics
	\begin{equation}
		\langle\alpha| \alpha \rangle \ge 0,
	\end{equation}
	comes from the probability interpretation of quantum mechanics.

	\subsection{Probability postulate}

	The probability postulate can not be proved right nor wrong. So how can we use probability theory to study the world?
	%我想问的问题是：“概率论的具体问题不符合概率论的公理或假设”这一点是没办法验证的（我们想要证伪嘛）。我们在没有进行这样一个验证的情况下，为什么就可以用概率论了，而且看起来应用的还不错。

	\subsection{(1.4.46) and (1.4.47)}

	Which is larger? (1.4.47) should be larger, because there's more terms in the summation.

	\subsection{Translation operator}

	Suppose a translation operator is given by
	\begin{equation}
		\hat{\mathscr{T}}(\dd \bfx') = 1-i\hat{\bfK}\cdot \dd\bfx',
	\end{equation}
	where $\hat{\bfK} = \hat{K}_j$, $j = 1,2,3$, is a vector with each component as Hermitian operator, $\dd \bfx' = \dd x'_i$, $i = 1,2,3$, is an infinitesimal displacement vector, which is just an array of numbers instead of operators.
	\begin{align*}
		[\hat{\bfx}, \hat{\mathscr{T}}(d\bfx')]_i &= -i (\hat{\bfx} \bfK\cdot \dd\bfx' - \bfK\cdot d\bfx'\hat{\bfx})_i \\
		&= -i(\hat{x}_i \hat{K}_j \dd x'_j - \hat{K}_j dx'_j \hat{x}_i) \\
		&= -i([\hat{x}_i, \hat{K}_j]) \dd x'_j,
	\end{align*}

	Also we know that
	\begin{align*}
		[\hat{\bfx}, \hat{\mathscr{T}}(d\bfx')]_i &= \dd x'_i \\
		&= \dd x'_j \delta_{ij},
	\end{align*}
	Thus we have the commutation relation between $\hat{x}_i$ and $\hat{K}_j$
	\begin{equation}
		[\hat{x}_i, \hat{K}_j] = i \delta_{ij}.
	\end{equation}

	\subsection{Complete set of compatible operators}

	If we are given a CSCO, we can choose a basis for the space of states made of common eigenvectors of the corresponding operators. We can uniquely identify each eigenvector by the set of eigenvalues it corresponds to.

	Why?

	\subsection{Typo a line above (1.4.57)}

	The value of $\lambda$ is actually
	\begin{equation}
		\lambda = - \frac{\langle \beta | \alpha \rangle}{\langle \beta | \beta \rangle}.
	\end{equation}

	\subsection{Galilean Invariance in Schrodinger equation}

	Consider a Galileo transformation:
	\begin{align*}
		x &= x' + v t', \\
		t &= t',
	\end{align*}
	then
	\begin{equation}
		f(x',t') \to f(x-vt,t),
	\end{equation}
	If we do the partial derivatives, then we will find
	\begin{align}
		\frac{\partial f}{\partial x'} &= \frac{\partial f }{\partial x}, \\
		\frac{\partial f}{\partial t'} &= \frac{\partial f }{\partial t } + v \frac{\partial f }{\partial t }, \label{Galileo_t_derivative}
	\end{align}
	The reason that $(\ref{Galileo_t_derivative})$ has a plus $v \partial_x f$ term is that when we do partial derivative $\partial_{t'}$, what we really want to do is to do time derivative only on the second argument in $f(x-vt,t)$, without touching the $t$ in the first argument. However, if we simply do $\partial_t f$ what we will get is actually $\partial_{t'} f - v \partial_x f$
	\begin{equation*}
		\partial_t f(x-vt,t) = \partial_{t'} f(x-vt, t') - v\partial_x f(x-vt,t),
	\end{equation*}
	or
	\begin{equation}
		\partial_{t'} f(x', t') = \partial_t f(x-vt,t) + v\partial_x f(x-vt,t).
	\end{equation}
	Thus we have
	\begin{align}
		\partial_{x'} &= \partial_x, \\
		\partial_{t'} &= \partial_t + v \partial_x.
	\end{align}

	\subsection{Residue Theorem and A Line Integral}

	We want to evaluate this integral
	\begin{equation}
		\int_0^{\infty} \frac{\ln x}{x^2 + a^2} dx,
	\end{equation}
	where $a>0$. To do it, we loop around the entire complex plane in the following way:

	see the picture.

	Therefore the contour integral can be written as
	\begin{align*}
		\int_0^{\infty} \frac{\ln x}{x^2 + a^2} dx &= \int_{\delta}^{R} \frac{\ln x}{x^2 + a^2} dx + \int_{C_R} \frac{\ln z}{z^2 + a^2} dz + \int^{\delta}_{R} \frac{\ln x + 2\pi i}{x^2 + a^2} dx + \int_{C_{\delta}} \frac{\ln z}{z^2 + a^2} dz \\
		&= 2\pi i \sum_{\mathbb{C}} \Res\left(\frac{\ln z}{z^2 + a^2}\right) \\
		&= 2\pi i \left(\frac{\ln a + i\pi/2}{2ia} + \frac{\ln a + i3\pi/2}{-2ia} \right) \\
		&= -i \frac{\pi^2}{a}.
	\end{align*}
	Since $\lim_{z\to0} z \ln z = 0$ and $\lim_{z \to \infty} \ln z/z = 0$, we eliminate two circle integrals
	\begin{align*}
		\int_{C_{\delta}} \frac{\ln z}{z^2 + a^2} dz &= 0,\\
		\int_{C_R} \frac{\ln z}{z^2 + a^2} dz &= 0,
	\end{align*}
	Therefore, after taking limit $R\to \infty$ and $\delta \to 0$, we obtain
	\begin{equation}
		\int_{0}^{\infty} \frac{\ln x}{x^2 + a^2} dx - \int_{0}^{\infty} \frac{\ln x + 2\pi i}{x^2 + a^2} dx = -i\frac{\pi^2}{a}.
	\end{equation}
	Although the integrals along both the banks of the branch cut are related to the integral we want to compute, but they cancel out each other and leave an integral which is not the one we want
	\begin{equation}
		\int_0^{\infty} \frac{1}{x^2 + a^2} = \frac{\pi}{2a}.
	\end{equation}

	On the other hand, this indicates us that if we want to compute $\int_0^{\infty} f(x) \ln x dx$, we should consider the complex integral $\oint_C f(z) \ln^2 z dz$, because in this case function $\ln^2 z$ on the two banks of branch cut partially cancel out, and leave the $\ln x$ term which is what we want.
	\begin{align*}
		\int_{0}^{\infty} \frac{\ln^2 x}{x^2 + a^2} dx - \int_{0}^{\infty} \frac{(\ln x + 2\pi i)^2}{x^2 + a^2} dx &= 2\pi i \sum_{\mathbb{C}} \Res\left(\frac{(\ln z)^2}{z^2 + a^2}\right) \\
		&= 2\pi i \left(\frac{(\ln a + i \pi/2)^2}{2ia} + \frac{(\ln a + i 3\pi/2)^2}{-2ia}\right) \\
		&= -i \frac{2\pi^2\ln a}{a} + \frac{2\pi^3}{a}.
	\end{align*}
	Thus
	\begin{equation}
		-4\pi i \int_{0}^{\infty} \frac{\ln x}{x^2 + a^2} dx + 4 \pi^2 \int_0^{\infty} \frac{1}{x^2 + a^2} dx = -i \frac{2\pi^2\ln a}{a} + \frac{2\pi^3}{a}.
	\end{equation}
	Therefore, we compute the integral
	\begin{equation}
		\int_{0}^{\infty} \frac{\ln x}{x^2 + a^2} dx = \frac{\pi}{2a}\ln a.
	\end{equation}

	\subsection{Order $\ln x$ and $-1/x$ at $x=0$}

	Consider the domain $x \in (0,1)$ , then we want to compare $\ln x$ and $-1/x$
	\begin{equation}
		r(x) = \frac{\ln x}{-1/x} = \frac{|\ln x|}{|1/x|} = -x\ln x,
	\end{equation}
	First we notice that $r(x) = -x \ln x > 0$ if $x \in (0,1)$.  Second, we do the derivative of this ratio and get
	\begin{equation}
		\frac{d r(x)}{dx} = -(\ln x +1),
	\end{equation}
	where $r'(x) > 0$ if $x < 1/e$, $r'(x) < 0$ if $x > 1/e$, $r'(x) = 0$ if $x = 1/e$, so $r(x)$ takes maximum at $x = 1/e$, and $r(1/e) = 1/e$,
	\begin{equation}
		0< r(x) < 1/e < 1.
	\end{equation}
	Therefore
	\begin{equation}
		|\ln x| < |- 1/x|, \quad x \in (0,1),
	\end{equation}
	or equivalently
	\begin{equation}
		\ln x > - 1/x, \quad x \in (0,1).
	\end{equation}

	\subsection{Saddle point approximation}

	Suppose we want to evaluate the following integral
	\begin{equation}
		I = \int_{-\infty}^{\infty} dx \e^{- f(x)}
	\end{equation}
	where
	\begin{equation}
		\lim_{x \to \pm \infty}f(x) = \infty
	\end{equation}
	Since the negative exponential function vanished very quickly when $f(x)$ becomes large, we only need to look at the contribution when $f(x)$ is at its minima. We can expand $f(x)$ around its minima $x_0$
	\begin{equation}
		f(x) = f(x_0) + \half f''(x_0)(x-x_0)^2 + \dots
	\end{equation}
	Then the integral can be written as
	\begin{align*}
		I &\approx \int_{-\infty}^{\infty} dx \exp[{- f(x_0) - \half f''(x_0)(x-x_0)^2})] \\
		&= \e^{-f(x_0)} \int_{-\infty}^{\infty} dx \exp[{- \half f''(x_0)(x-x_0)^2})] \\
		&= \e^{-f(x_0)} \sqrt{\frac{2\pi}{f''(x_0)}}
	\end{align*}
	If $f(x)$ has several local minima $\{x_i\}$, we should sum over all the contribution from the minima
	\begin{equation}
		I \approx \sum_i \e^{-f(x_i)} \sqrt{\frac{2\pi}{f''(x_i)}}
	\end{equation}

	\subsection{Qubit}
	All qubit states $\omega$ may be represented as $2 \times 2$ matrices
	\begin{gather}
		\rho = \frac{1}{2}
		\begin{pmatrix}
			1 + x_3 & x_1 - i x_2 \\
			x_1 + i x_2 & 1 - x_3
		\end{pmatrix} , \quad \underline{x} \in \mathbb{R}^3\\
		\rho \ge 0 , \quad \operatorname{tr}(\rho) = 1
	\end{gather}
	\begin{proposition}
		Show $\rho \ge 0 \Leftrightarrow |\underline{x}| \le 1$. Thus $\underline{x} = (x_1, x_2, x_3)$ is in ball of radius 1.
		\begin{equation}
			\rho = \frac{1}{2}(\mathbb{I} + x_1 \sigma_x + x_2 \sigma_y + x_3 \sigma_z)
		\end{equation}
	\end{proposition}
	\begin{proof}
		the eigenvalues of $\rho$ is $\frac{1}{2} (1 \pm |\underline{x}|)$. For $\rho \ge 0$, $\exists X \in \bbC^{2 \times 2}$, such that $\rho = X^H X$.
		Equivalently speaking, the eigenvalues of $\rho$ are all greater than or equal to 0. Therefore $|\underline{x}| \le 1$.
	\end{proof}
	For pure states of a qubit, the possible vector $\underline{x}$ is in a sphere of radius 1, which is called the Bloch sphere. But unfortunately the idea of Bloch sphere in 2D quantum system cannot be generalized to higher dimension.

	\subsection{The improved Bohr-Sommerfeld quantization formula}
	\begin{theorem}
		From the boundary condition (or connection condition) of WKB approximation at the classical turning points, we are able to write down the semiclassical quantization formula as follows
		\begin{equation}
			\oint p(x) \dd x = 2 \pi \hbar (n + \mu/4) \quad \text{with } \ \mu = N_{\text{soft}} + 2N_{\text{hard}}
		\end{equation}
		where $n = 0,1,2,\dots$, and $\mu$ is called \textit{Maslov index} and counts the number of classical turning points with smooth potential (soft wall) plus twice the number of classical turning points with Dirichlet boundary conditions (hard wall).
	\end{theorem}

	\subsection{Landau level for a charged particle in unifrom magnetic field}

	A charged paticle is moving in the presence of a uniform magnetic field in the $z$-direction ($\bfB = B \hat{z}$).
	Define the kinematical (or mechanical) momentum
	\begin{equation}
		\mathbf{\Pi} = m \ddt{\bfx} = \bfp - \frac{e \bfA}{c}
	\end{equation}
	We can compute the commutator between componets of kinematical momentum
	\begin{align*}
		[\Pi_i, \Pi_j] &= [p_i - e A_i(x) /c, p_j - e A_j(x) /c] \\
		&= [p_i, p_j] -\frac{e}{c} \{[A_i(x), p_j] + [p_i, A_j(x)]\} + \frac{e^2}{c^2}[A_i(x),A_j(x)] \\
		&= \frac{i \hbar e}{c}\left[ \del{A_j}{x_i} - \del{A_i}{x_i} \right] \\
		&= \frac{i \hbar e}{c} B_{ij} = \frac{i \hbar e}{c} \eps_{ijk} B_k \numberthis \label{eq: commutator of kinematical momentum}
	\end{align*}
	where $B_{ij} = \eps_{ijk} B_k$ is an antisymmetric tensor with matrix elements
	\begin{gather}
		B_{ij} = \pdv{A_j}{x_i} - \pdv{A_i}{x_i} \doteq
		\begin{pmatrix}
			0 & B_z & -B_y \\
			-B_z & 0 & B_x \\
			B_y & -B_x & 0
		\end{pmatrix} \\
		B_k = \eps_{ijk} \pdv{A_j}{x_i} = \thalf \eps_{ijk} B_{ij}
	\end{gather}
	We also note the commutators,
	\begin{equation}
		[x_i, p_j] = \frac{i \hbar}{m} \delta_{ij}
	\end{equation}
	The kinematical momentum commutators \eqref{eq: commutator of kinematical momentum} are proportional to the components of the magnetic field. Notice that this result holds regardless of the nature of the magnetic field (nonuniform, time-dependent, etc), with or without an electric field. If the magnetic field vanished, then the components of velocity commute with one another, as is obvious since in that case $\bfp = m \bfv$, and since $[p_i, p_j] = 0$.

	As assumed before, now we apply uniform magnetic field along $z$ axis, such that $\bfB = B \vu{z}$, the commutators \eqref{eq: commutator of kinematical momentum} become
	\begin{align*}
		[\Pi_x, \Pi_y] &= \frac{i \hbar e B}{c} \eps_{ijk} = i \hbar \sign(e) m \omega, \\
		[\Pi_x, \Pi_z] &= [\Pi_y, \Pi_z] = 0,
	\end{align*}
	where $\omega = \abs{e}B/mc$ is the frequency of the orbital gyration of the particle (the \textit{gyrogrequency}).
	Next we do a change of variable $X = \sign(e)\Pi_y/m\omega, P = \Pi_x$, with commutator $[X,P] = i\hbar$, such that we can rewrite the Hamiltonian as
	\begin{equation}
		H = \frac{\Pi^2}{2m} = \frac{P^2}{2m} + \thalf m \omega^2 X^2 + \frac{\Pi_z^2}{2m}
	\end{equation}
	The perpendicular kinetic energy appears as a harmonic oscillator, while the parallel kinetic energy appears as a one-dimensional free partice. Therefore, the energy spectrum of $H$ is immediately

	\begin{equation}
		E = (n+\thalf)\hbar \omega + \frac{\Pi_z^2}{2m}.
	\end{equation}

	\subsection{Order of even permutation group (alternating groyp)}

	The subset of the permutation group $S_n$ formed by even permutations is a group, called the alternating group $A_n$. We can check that it satisfies the definition of a group
	\begin{itemize}
		\item The identity is the do-nothing permutation $\sigma = ()$, and its determinant is 1 and $\sign (()) = 1$, that is $()$ is even.
		\item The composition of two even permutations is even, thus closure is satisfied.
		\item The inverse of an even permutation must be even. To show this, we know
		\begin{equation}
			P_{\sigma}^{\top} P_{\sigma} = I,
		\end{equation}
		so $\det(P_{\sigma}^{\top}) = \det(P_{\sigma})$ implies $\det(P_{\sigma}^{\top}) = 1$ if $\det(P_{\sigma}) = 1$.
	\end{itemize}
	The size of $A_n$ is $\thalf n!$, since for every even permutation, one can uniquely associate an odd one by exchanging the first two elements.
	\begin{proof}
			Say $A_n = {a_i}$, and $a_i$ is the element of alternating group. $E$ is the transposition that exchanges the first two elements, which means $E a_i$ is odd.
			Next we want to show that $E a_i \neq E a_j$ if $a_i \neq a_j$. Well, one can immediately prove this by saying that permutations are bijective, thus injective. Or we can do the following: Suppose $E a_i = E a_j$ for some $a_i \neq a_j$, then $E E a_i = E E a_j$, which implies $a_i = a_j$, because $E^2 = I$. Contradiction.
	\end{proof}

	\subsection{Cross product}

	Actually, there does not exist a cross product vector in space with more than 3 dimensions. The fact that the cross product of 3 dimensions vector gives an object which also has 3 dimensions is just pure coincidence. However, we can always define the cross product tensor in any dimension $n$, such that the cross product of $\bfa$ and $\bfb$ is defined as
	\begin{equation}
		c_{ij} = a_i b_j - a_j b_i. \label{def: cross product tensor}
	\end{equation}
	Therefore the cross product tensor is of rank 2 and it is antisymmetric. So the cross product tensor has $n(n-1)/2$ independent elements.

	The cross product in 3 dimensions is actually a tensor of rank 2 with 3 independent coordinates. We can create a 3-vector in 3 dimensions to represent these 3 independent elements.
	\begin{equation}
		c_{ij} =
		\begin{pmatrix}
			0 & c_3 & -c_2 \\
			-c_3 & 0 & c_1 \\
			c_2 & -c_1 & 0
		\end{pmatrix}
		= \eps_{ijk} c_k, \label{eq: cross product tensor}
	\end{equation}
	where $\bfc$ is the cross product vector of $\bfa$ and $\bfb$ in 3 dimensions and defined as
	\begin{equation}
		c_i = (\bfa \times \bfb)_i = \eps_{ijk} a_j b_k. \label{def: cross product vector}
	\end{equation}
	In addition to \eqref{eq: cross product tensor}, we can also express the cross product vector $\bfc$ interms of the cross product tensor $c_{ij}$
	\begin{equation}
		c_i = \thalf \eps_{ijk} c_{jk}
	\end{equation}
	Using the definition \eqref{def: cross product tensor} and \eqref{def: cross product vector}, we can immediately show this.
	\begin{align*}
		\thalf \eps_{ijk} c_{jk}
		&= \thalf \eps_{ijk} (a_j b_k - a_k b_j) \\
		&= \thalf \eps_{ijk} a_j b_k - \thalf \eps_{ijk} a_k b_j \\
		&= \eps_{ijk} a_j b_k = c_i.
	\end{align*}

	\subsection{Probability flux in the presence of electromagnetic field}

	We now study Schr\"{o}dinger's wave equation with $\phi$ and $\bfA$.
	\begin{align*}
		H \psi &= \frac{1}{2m}\qty[-i\hbar \grad - \frac{e \bfA(\bfx) }{c}] \vdot \qty[-i\hbar \grad - \frac{e \bfA(\bfx) }{c}] \psi(\bfx,t) + e \phi(\bfx) \psi(\bfx,t)\\
		&= -\frac{\hbar^2}{2m} \laplacian \psi + \frac{i \hbar e}{2mc} \div(\bfA \psi) + \frac{i \hbar e}{2mc} \bfA \vdot (\grad{\psi}) + \qty[\frac{e^2 A^2}{2mc^2} + e \phi] \psi \\
		&= -\frac{\hbar^2}{2m} \laplacian \psi + \frac{i \hbar e}{2mc} (\div{\bfA}) \psi + \frac{i \hbar e}{mc} \bfA \vdot (\grad{\psi}) + \qty[\frac{e^2 A^2}{2mc^2} + e \phi] \psi, \\
		(H \psi)^* &= -\frac{\hbar^2}{2m} \laplacian \psi^* - \frac{i \hbar e}{2mc} (\div{\bfA}) \psi^* - \frac{i \hbar e}{mc} \bfA \vdot (\grad{\psi^*}) + \qty[\frac{e^2 A^2}{2mc^2} + e \phi] \psi^*\\
	\end{align*}
	The probability density $\rho = \abs{\psi}^2$, we want to find the corresponding probability flux $\bfj$ such that the continuity equation is satisfied
	\begin{equation}
		\pdv{\rho}{t} + \div{\bfj} = 0.
	\end{equation}
	For this purpose we rewrite $\pdv{\rho}{t}$ using Sch\"{o}dinger's wave equation
	\begin{align*}
		\pdv{\rho}{t} &= \psi^* \pdv{\psi}{t} + \psi \pdv{\psi^*}{t} = \frac{1}{i \hbar} (\psi^* H \psi - \psi H \psi^*)\\
		&= \frac{i \hbar}{2m}(\psi^* \laplacian \psi + \psi \laplacian \psi^*) + \frac{e}{2mc}(\div{\bfA})(\psi^* \psi + \psi \psi^*) + \frac{e}{mc} \bfA \vdot (\psi^* \grad{\psi} + \psi \grad{\psi^*}) \\
		&= \frac{i \hbar}{2m} \div(\psi^* \grad{\psi} - \qcc) + \frac{e}{mc}\qty[(\div{\bfA}) \abs{\psi}^2 + \bfA \vdot \grad{\abs{\psi}^2}] \\
		&= - \div[\qty(\frac{\hbar}{m}) \Im{\psi^* \grad{\psi}} - \qty(\frac{e}{mc}) \bfA \abs{\psi}^2].
	\end{align*}
	Thus we have the probability flux
	\begin{align*}
		\bfj &= \qty(\frac{\hbar}{m}) \Im{\psi^* \grad{\psi}} - \qty(\frac{e}{mc}) \bfA \abs{\psi}^2 \\
		&= \qty(\frac{\hbar}{m}) \Im{\psi^* \qty[\grad - \qty(\frac{i e}{\hbar c}) \bfA] \psi},
	\end{align*}
	which is just what we expect from the substitution
	\begin{equation}
		\grad \to \grad - \qty(\frac{i e}{\hbar c}) \bfA.
	\end{equation}

	\subsection{Electromagnetic gauge transformations are canonical transformations}

	Consider a electromagnetic gauge transformation
	\begin{gather}
		\bfA' = \bfA + \grad \Lambda(\bfx, t), \\
		\phi' = \phi - \frac{1}{c} \pdv{t} \Lambda(\bfx, t)
	\end{gather}
	We claim that this gauge transformation is equivalent to a canonical transformation of type
	\begin{equation}
		L(q, \dot{q}, t)' = L(q, \dot{q}, t) + \dv{f(q,t)}{t},
	\end{equation}
	which generates new canonical variables
	\begin{gather}
		Q = q, \\
		P = \pdv{L'}{\dot{q}} = p + \pdv{f}{q}.
	\end{gather}
	To show this, we substitude the electromagnetic potential into the Lagrangian
	\begin{equation}
		L = \thalf m \dot{\bfx}^2 + \frac{e}{c} \dot{\bfx} \vdot \bfA - e \phi,
	\end{equation}
	which yields
	\begin{align*}
		L' &= \thalf m \dot{\bfx}^2 + \frac{e}{c} \dot{\bfx} \vdot (\bfA + \grad \Lambda) - e \qty(\phi - \frac{1}{c} \pdv{t} \Lambda) \\
		&= L + \frac{e}{c} \dv{\Lambda}{t}.
	\end{align*}

	\subsection{One-dimensional quantum ring}
	Consider a particle living in a circular ring with fixed radius $R$. Recall the gradient in cylindrical coordinates
	\begin{equation}
		\grad = \pdv{\rho} \vu*{\rho} + \frac{1}{\rho}\pdv{\phi} \vu*{\phi} + \pdv{z} \vu{z},
	\end{equation}
	and in our ring case, only the $\vu*{phi}$ component matters, so we obtain a reduced one-dimensional Schr\"{o}dinger's equation for a free particle
	\begin{equation}
		- \frac{\hbar^2}{2mR^2} \dv[2]{\phi} \psi(\phi) = E \psi(\phi). \label{eq: free particle in a ring}
	\end{equation}
	with periodic boundary condition
	\begin{equation}
		\psi(\phi + 2 \pi) = \psi(\phi).
	\end{equation}
	The general solution to \eqref{eq: free particle in a ring} is a plane wave $\e^{i n \phi}$, to satisfy the boundary condition we require $n \in \bbZ$. Therefore we have the eigenfunctions and energies labelled by the quantum number $n$
	\begin{align}
		\psi_n(\phi) &= \e^{i n \phi}, \\
		E_n &= \frac{\hbar^2 n^2}{2mR^2},
	\end{align}
	where $n \in \bbZ$.

	Now imagine that along the axis of the circle runs a solenoid of a radius $a < R$, carrying a steady electric current $I$. If the solenoid is long enough the magnetic field $\bfB = B \vu{z}$ is uniform inside and zero outside the solenoid. But the vector potential is not zero!
	\begin{equation}
		\bfA = \frac{\Psi}{2 \pi \rho} \vu*{\phi},
	\end{equation}
	where $Psi = \pi a^2 B$ is the magnetic flux through the solenoid. In the presence of this magnetic field, the kinematic momentum changes $\bfp \to \bfp - \frac{e}{c} \bfA$, or in terms of the gradient $\grad \to \grad - \tfrac{ie}{\hbar c} \bfA$, such that we have the Hamiltonian
	\begin{equation}
		H = \frac{\qty(\bfp - \frac{e}{c}\bfA)^2}{2m}.
	\end{equation}
	Since we only care about the $\vu*{\phi}$ component, the Hamiltonian can be written as
	\begin{equation}
		H = \frac{\hbar^2(\hat{l}_z-\alpha)^2}{2mR^2}
	\end{equation}
	where $\hat{l}_z = -i \dv{\phi}$ in $\phi$ representation, and $\alpha = ea^2B/2\hbar c$. The periodic boundary condition is still true even in the presence of magnetic field, because we still have the rotational symmetry, which means the eigenfunction $\psi(\phi) = \e^{i n \phi}$ with $n \in \bbZ$ is still true.
	\begin{gather}
		\psi_n(\phi) = \e^{i n \phi}, \\
		E_n = \frac{\hbar^2 (n- \alpha)^2}{2mR^2}.
	\end{gather}
	Alternatively one may choose a unitary gauge transformation $U(\hat{\phi})$ to solve the Schr\"{o}dinger's equation with magnetic field
	\begin{equation}
		U(\hat{\phi}) = \exp(-\frac{-i e}{\hbar c}\int^\phi \bfA \vdot \dd \bfl) = \e^{-i \alpha \hat{\phi}}
	\end{equation}
	which transforms the Hamiltonian to a field-free Hamiltonian in the following way
	\begin{equation}
		H' = U H H^{\dagger} = \e^{-i \alpha \hat{\phi}} \frac{\hbar^2(\hat{l}_z-\alpha)^2}{2mR^2} \e^{i \alpha \hat{\phi}} = \frac{\hbar^2 \hat{l}_z^2}{2mR^2},
	\end{equation}
	and we can think of $\e^{-i \alpha \hat{\phi}}$ as the translation operator for angular momentum $\hat{l}_z$, such that
	\begin{align}
		\e^{-i \alpha \hat{\phi}} \ \hat{l}_z \ \e^{i \alpha \hat{\phi}} &= \hat{l}_z + \alpha, \\
		\e^{-i \alpha \hat{\phi}} \ket{l_z} &= \ket{l_z + \alpha},
	\end{align}
	where $\ket{l_z}$ is the eigenket of $\hat{l}_z$.
	Given the original Schr\"{o}dinger equation $H \psi = E \psi$, we can rewrite it as
	\begin{gather}
		H' \psi' = U H U^{\dagger} U \psi = E \psi', \\
		- \frac{\hbar^2}{2m R^2} \dv[2]{\phi} \psi' = E \psi'
	\end{gather}
	The eigenenergies are conserved under gauge transformation. The general solution for $\psi'$ is $\e^{i k \phi}$, but $k \in \bbZ$ isn't necessary since the boundary conditions changes.
	If the periodic boundary conditions were true for the original wavefunction, such that
	\begin{equation}
		\psi(\phi + 2\pi) = \psi(\phi),
	\end{equation}
	then for the new wavefunction after gauge transformation, we have the ``twristed'' boundary conditions
	\begin{equation}
		\psi'(\phi + 2 \pi) = \e^{-i \alpha(\phi + 2 \pi)} \psi(\phi + 2\pi) = \e^{-i\alpha 2\pi} \psi'(\phi).
	\end{equation}
	Plug in the general solution for $\psi' = \e^{i k \phi}$,
	\begin{equation}
		\psi'(\phi + 2\pi) = \e^{i k (\phi + 2\pi)} = \e^{i k 2\pi} \psi'(\phi)
	\end{equation}
	we find the constraint of $k$
	\begin{equation}
		k = n - \alpha, \quad n \in \bbZ.
	\end{equation}
	Then $\psi'(\phi)$ and $\psi(\phi)$ are given by
	\begin{align}
		\psi'_n(\phi) &= \e^{i k \phi} = \e^{i(n-\alpha)\phi}, \\
		\psi_n(\phi) &= U^{\dagger} \psi'(\phi) = \e^{in \phi}.
	\end{align}
	Here we see that the original eigenfunction is indeed the same as the eigenfunction as the free particle. The eigenenergies are
	\begin{equation}
		E_n = \frac{\hbar^2 k^2}{2mR^2} = \frac{\hbar^2 (n-\alpha)^2}{2mR^2}.
	\end{equation}
	The kinetic angular momentum in presence of magnetic field is $\hbar k = \hbar(n - \alpha)$.

	\subsection{Quantum harmonic oscillator}

	The operator method can also be used to obtain the energy eigenfunctions in position space. Let us start with the ground state defined by
	\begin{equation}
		a \ket{0} = 0,
	\end{equation}
	which, in the $x$-representation, reads
	\begin{equation}
		\mel{x'}{a}{0} = \sqrt{\frac{m \omega}{2 \hbar}} \mel{x'}{x - \frac{i p}{m \omega}}{0} = 0.
	\end{equation}
	Recalling
	\begin{equation}
		\mel{x'}{p}{\alpha} = -i \hbar \dv{x'} \braket{x'}{\alpha},
	\end{equation}
	we can regard this as as differential equation for the ground state wave function $\braket{x'}{0}$
	\begin{equation}
		\qty(x' + x_0^2 \dv{x'}) \braket{x'}{0} = 0, \label{eq: ground state of harmonic oscillator}
	\end{equation}
	where we have introduced
	\begin{equation}
		x_0 = \sqrt{\frac{\hbar}{m \omega}},
	\end{equation}
	which sets the length scale of the oscillator. We see that the normalized solution to \eqref{eq: ground state of harmonic oscillator} is
	\begin{equation}
		\braket{x'}{0} = \frac{1}{\pi^{1/4}\sqrt{x_0}} \exp[-\half \qty(\frac{x'}{x_0})^2].
	\end{equation}
	The corresponding differential operators of $a$ and $a^{\dagger}$ are
	\begin{align}
		a &= \frac{1}{\sqrt{2} x_0} \qty(x' + x_0^2 \dv{x'}), \\
		a^{\dagger} &= \frac{1}{\sqrt{2} x_0} \qty(x' - x_0^2 \dv{x'}).
	\end{align}
	Thus We can also obtain the energy eigenfunctions for excited states
	\begin{align*}
		\braket{x'}{n} &= \mel{x'
		}{\frac{(a^{\dagger})^n}{\sqrt{n!}}}{0} \\
		&= \frac{1}{\pi^{1/4} \sqrt{2^n n!}  x_0^{n+1/2}} \qty(x' - x_0^2 \dv{x'})^n \exp[-\half \qty(\frac{x'}{x_0})^2]. \numberthis
	\end{align*}

	\subsection{Completeness of coherent states}
	Coherent states are overcomplete, and
	\begin{equation}
		\int_{\mathbb{C}} \dd^2{\lambda} \dyad{\lambda}{\lambda} = \pi.
	\end{equation}
	\begin{proof}
		Note that
		\begin{equation}
			\ket{\lambda} = \e^{-\abs{\lambda}^2/2} \sum_{n=0}^{\infty} \frac{\lambda^n}{\sqrt{n!}} \ket{n}.
		\end{equation}
		Thus
		\begin{align*}
			\int_{\mathbb{C}} \dd^2{\lambda} \dyad{\lambda}{\lambda} &= \int_{\mathbb{C}} \dd^2{\lambda} \e^{-\abs{\lambda}^2} \sum_{m,n} \frac{(\lambda^*)^n \lambda^m}{\sqrt{n!m!}} \dyad{m}{n} \\
			&= \sum_{m,n} \frac{\dyad{m}{n}}{\sqrt{n!m!}} \int_{\mathbb{C}} \dd^2{\lambda} \e^{-\abs{\lambda}^2} (\lambda^*)^n \lambda^m,
		\end{align*}
		where the integral over $\lambda$ can be evaluated as
		\begin{align*}
			\int_{\mathbb{C}} \dd^2{\lambda} \e^{-\abs{\lambda}^2} (\lambda^*)^n \lambda^m &= \int_0^{\infty} \dd{r} r \e^{-r^2} r^{m+n} \int_0^{2\pi} \dd{\varphi} \e^{i(m-n) \varphi} \\
			&= 2\pi \delta_{mn} \int_0^{\infty} \dd{r} r \e^{-r^2} r^{m+n} \\
			&= \sum_{m,n} \frac{\pi \delta_{mn}}{\sqrt{n!m!}} \dyad{m}{n} \int_0^{\infty} 2r \dd{r} \e^{-r^2} r^{m+n} \\
			(x = r^2) &= \sum_n \frac{\pi}{n!} \dyad{n} \int_0^{\infty} \dd{x} x^n \e^{-x} \\
			&= \sum_n \pi \dyad{n} = \pi.
		\end{align*}
	\end{proof}

	\subsection{Coherent state as a Gaussian wave packet with minimum uncertainty}

	In position representation, the wavefunction of coherent state can be written as
	\begin{align*}
		\psi_{\lambda}(x') &= \braket{x'}{\lambda} = \mel{x}{\exp(-1/2 \abs{\lambda}^2 + \lambda a^{\dagger})}{0} \\
		&= \e^{-1/2 \abs{\lambda}^2} \mel{x'}{\exp[\lambda \sqrt{\frac{m\omega}{2\hbar}} \qty(x - \frac{ip}{m\omega})]}{0} \\
		&= \e^{-1/2 \abs{\lambda}^2} \exp[\lambda \sqrt{\frac{m\omega}{2\hbar}} \qty(x' - \frac{i}{m\omega} \frac{\hbar}{i}\dv{x'})] \braket{x'}{0} \\
		&= \e^{-1/2 \abs{\lambda}^2} \exp[\lambda \frac{1}{\sqrt{2} x_0} \qty(x' - x_0^2\dv{x'})] \psi_0(x') \\
		&= A \e^{-1/2 \abs{\lambda}^2} \exp[\lambda/\sqrt{2} \qty(y - \dv{y})] \e^{-y^2/2} \\
		&= A \exp(-\frac{\abs{\lambda}^2}{2} - \frac{\lambda^2}{4}) \e^{\frac{\lambda}{\sqrt{2}} y} \e^{\frac{\lambda}{\sqrt{2}} \dv{y}} \e^{-y^2/2}
	\end{align*}


	\subsection{Properties of Pauli matrices}

	The three Pauli matrices are defined as follows
	\begin{equation}
		\sigma_x = \mqty(\pmat{1}), \quad \sigma_y = \mqty(\pmat{2}), \quad \sigma_z = \mqty(\pmat{3}),
	\end{equation}
	and we can compute their commutators and anticommutators
	\begin{align}
		[\sigma_j, \sigma_k] &= 2i \eps_{jkl} \sigma_l, \\
		\{ \sigma_j, \sigma_k \} &= 2 \delta_{jk},
	\end{align}
	All three matrices multiplied by $i$ form a basis of $\mfs\mfu(2)$ Lie algebra.
	Also if we sum up the commutator and the anticommtator for $\sigma_{j,k}$ we have
	\begin{equation}
		\sigma_j \sigma_k = \delta_{jk} + i \eps_{jkl} \sigma_l.
	\end{equation}

	\subsection{Orthogonal curvilinear coordinates}

	In an $n$-dimensional vector space, the corresponding catesian coordinates are denoted by $\{ x'^k \}$, $k = 1,2, \dots, n$. We define a set of curvilinear coordinates $\{ x^k \}$, such that
	\begin{equation}
		x^k = x^k( \{ x'^k \} ),
	\end{equation}
	whose coordinate surfaces are $x^k = \const$ In order that $\{ x^k \}$ are linear-independent, its determinant of Jacobian is nonzero
	\begin{equation}
		\det(\pdv{\bfx}{\bfx'}) \neq 0,
	\end{equation}
	where the matrix elements are
	\begin{equation}
		\pqty{\pdv{\bfx}{\bfx'}}_{k l} = \pdv{x^k}{x'^l}.
	\end{equation}
	Fo any point $\bfx$ in the vector space, if all coordinate surfaces are orthogonal to each other, then we call $\{ x^k \}$ a set of orthogonal coordinates.

	To determine if $\{ x^k \}$ are orthogonal, we can compute its squared infinitesimal distance
	\begin{align*}
		\dd[2]{s} &= \dd x'^k \dd x'^k \\
		&= \pdv{x'^k}{x^i} \pdv{x'^k}{x^j} \dd x^i \dd x^j \\
		&= g_{ij} \dd x^i \dd x^j,
	\end{align*}
	where the metric tensor is defined as
	\begin{equation}
		g_{ij} = \pdv{x'^k}{x^i} \pdv{x'^k}{x^j}.
	\end{equation}
	If $g_{ij} = g_{ii} \de_{ij}$, then $\{ x^k \}$ are orthogonal coordinates.
	\begin{example}
		Cylindrical coordinates:
		\begin{gather}
			x = r \cos \phi, \quad y = r \sin \phi, \quad z = z, \\
			\dd[2]{s} = \dd[2]{x} + \dd[2]{y} + \dd[2]{z} = \dd[2]{r} + r^2 \dd[2]{\phi} + \dd[2]{z}.
		\end{gather}
		Spherical coordinates:
		\begin{gather}
			x = r \sin \theta \cos \phi, \quad y = r \sin \theta \sin \phi, \quad z = r \cos \theta, \\
			\dd[2]{s} = \dd[2]{x} + \dd[2]{y} + \dd[2]{z} = \dd[2]{r} + r^2 \dd[2]{\theta} + r^2 \sin^2{\theta} \dd[2]{z}.
		\end{gather}
	\end{example}
	\subsection{Exterior derivative}

	\begin{definition}
		Differential forms provide an approach to multivariable calculus that is independent of coordinates, i.e., covariant. A differential form of order $p$, which is also denoted as a $p$-form or $p$-vector, can be thought as a volume element in $p$-dimensional space. In general, a $p$-form is an object that may be integrated over $p$-dimensional sets, and is homogeneous of degree $p$ in the coordinate differentials.

		In particular, $\{\dd x^i \}$, where $i = 1, 2, \dots, n$ form a complete basis of 1-form in $n$-dimensional space, and we should think of $\dd x^i$ as a vector instead of a poor differential scalar.
	\end{definition}

	\begin{definition}
		Exterior derivative is a function which maps a $p$-form into a $(p+1)$-form. Suppose $\alpha$ is a $p$-form, $\beta$ and $\gamma$ are $q$-forms, then
		\begin{enumerate}
			\item $\dd (\beta + \gamma) = \dd \beta + \dd \gamma$.
			\item $\dd (\alpha \wedge \beta) = (\dd \alpha) \wedge \beta + (-)^{p} \alpha \wedge (\dd \beta)$.
			\item $\dd (\dd \alpha) = 0$.
		\end{enumerate}
		Operation $\wedge$ is called wedge product or exterior product, which is antisymmetric in the sense of
		\begin{equation}
			\dd x^i \wedge \dd x^j = - \dd x^j \wedge \dd x^i,
		\end{equation}
		such that wedge product vanishes $\dd x^i \wedge \dd x^j = 0$ if $i = j$.
	\end{definition}

	Suppose $f$ is a smooth function of $\{ x^i \}$, i.e. a 0-form, then
	\begin{equation}
		\dd f = \pdv{f}{x^i} \dd x^i,
	\end{equation}
	$\dd f$ is called a 1-form. If we change the coordinates from $\{ x^i \}$ to $\{ y^i \}$, then
	\begin{equation}
		\dd f = \pdv{f}{y^i} \dd y^i.
	\end{equation}
	As we can see, $\dd f$ gives the gradient of $f$ written in a covariant way. If $\{ x^i \}$ are orthogonal coordinates, the corresponding orthonormal basis of 1-forms are $\{\sqrt{g_{ii}} \dd x^i \}$.
	In cylindrical coordinates:
	\begin{gather}
		\dd \rho \to \vu{e}_{\rho}, \quad \rho \dd \phi \to \vu{e}_{\phi}, \quad \dd z \to \vu{e}_{z}, \\
		\grad{f} = \pdv{f}{\rho} \vu{e}_{\rho} + \frac{1}{\rho}\pdv{f}{\phi} \vu{e}_{\phi} + \pdv{f}{z} \vu{e}_z.
	\end{gather}
	In spherical coordinates:
	\begin{gather}
		\dd r \to \vu{e}_r, \quad r \dd \theta \to \vu{e}_{\theta}, \quad r \sin \theta \dd \phi \to \vu{e}_{\phi}, \\
		\grad{f} = \pdv{f}{r} \vu{e}_r + \frac{1}{r}\pdv{f}{\theta} \vu{e}_{\theta} + \frac{1}{r \sin \theta} \pdv{f}{\phi} \vu{e}_{\phi}.
	\end{gather}
	Next we look at the exterior derivative of an arbitrary $p$-form $\alpha = \alpha_I \dd x^I$, where $I$ is a multi-index $\{ i_k \}$, $1 \le i_k \le n$ for $1 \le k \le p$, and $\dd x^I = \dd x^{i_1} \wedge \cdots \wedge \dd x^{i_p}$.
	\begin{equation}
		\dd \alpha = \pdv{\alpha_I}{x^i} \dd x^i \wedge \dd x^I.
	\end{equation}

	\begin{definition}
		For a $n$-dimensional vector space, the Hodge star operation $\star$ is a linear map, which maps a $p$-form to a $(n-p)$-form (Hodge dual)
		\begin{equation}
			\star (\dd x^{i_1} \wedge \cdots \wedge \dd x^{i_p}) = \frac{\sqrt{\abs{\det g}}~}{(n-p)!} g^{i_1 j_1} \cdots g^{i_p j_p} \eps_{j_1 \dots j_n} \dd x^{j_{p+1}} \wedge \cdots \wedge \dd x^{j_n}.
		\end{equation}
		where $\eps_{j_1 \dots j_n}$ is the Levi-Civita symbol, and $g^{ij}$ is the inverse metric tensor.
	\end{definition}

	In 3-dimensional orthogonal coordinates,
	\begin{align}
		\star~\dd x^i &= \frac{\sqrt{\det g}~}{g_{ii}} \dd x^I, \\
		\star~\dd x^I &= \frac{g_{ii}}{\sqrt{\det g}~} \dd x^i.
	\end{align}
	where $(i, I)$ are the even permutations of $(1,2,3)$.
	\begin{equation}
		\star 1 = \sqrt{\det g}~\dd x^1 \wedge \dd x^2 \wedge \dd x^3, \quad
		\star (\sqrt{\det g}~\dd x^1 \wedge \dd x^2 \wedge \dd x^3) = 1.
	\end{equation}
	Notice that $\sqrt{\det g}~\dd x^1 \wedge \dd x^2 \wedge \dd x^3$ is the differential volume element in 3D.
	\begin{example}
		\begin{enumerate}
			\item For cylindrical coordinates:
			\begin{gather*}
				\dd[2]{s} = \dd[2]{\rho} + \rho^2 \dd[2]{\phi} + \dd{z}, \quad \det g = \rho^2, \\
				\dd{u} = \pdv{u}{\rho} \dd{\rho} + \pdv{u}{\phi} \dd{\phi} + \pdv{u}{z} \dd{z},
			\end{gather*}
			\begin{align*}
				\star \dd{u} &= \sqrt{\rho^2} \qty(\pdv{u}{\rho} \dd{\phi} \wedge \dd{z} + \frac{1}{\rho^2} \pdv{u}{\phi} \dd{z} \wedge \dd{\rho} + \pdv{u}{z} \dd{\rho} \wedge \dd{\phi}) \\
				&= \rho \pdv{u}{\rho} \dd{\phi} \wedge \dd{z} + \frac{1}{\rho} \pdv{u}{\phi} \dd{z} \wedge \dd{\rho} + \rho \pdv{u}{z} \dd{\rho} \wedge \dd{\phi}.
			\end{align*}
			\item For spherical coordinates:
			\begin{gather*}
				\dd[2]{s} = \dd[2]{r} + r^2 \dd[2]{\theta} + r^2 \sin^2{\theta} \dd{z}, \quad \det g = r^4 \sin^2, \\
				\dd{u} = \pdv{u}{r} \dd{r} + \pdv{u}{\theta} \dd{\theta} + \pdv{u}{\phi} \dd{\phi},
			\end{gather*}
			\begin{align*}
				\star \dd{u} &= \sqrt{r^4 \sin^2{\theta}} \qty(\pdv{u}{r} \dd{\theta} \wedge \dd{\phi} + \frac{1}{r^2} \pdv{u}{\theta} \dd{\phi} \wedge \dd{r} + \frac{1}{r^2 \sin^2 {\theta}} \pdv{u}{\phi} \dd{r} \wedge \dd{\theta}) \\
				&= r^2 \sin^2{\theta} \pdv{u}{r} \dd{\theta} \wedge \dd{\phi} + \sin{\theta} \pdv{u}{\theta} \dd{\phi} \wedge \dd{r} + \frac{1}{\sin{\theta}} \pdv{u}{\phi} \dd{r} \wedge \dd{\theta}.
			\end{align*}
		\end{enumerate}
	\end{example}
	In 3-dimensional space, $\star\dd$ is the curl written in a covariant way, which can be seen from
	\begin{align*}
		\lefteqn{\star \dd(a_1 \dd{x^1} + a_2 \dd{x^2} + a_3 \dd{x^3})} \\
		=& \frac{1}{\sqrt{\det g}} \qty[\qty(\pdv{a_3}{x^2} - \pdv{a_2}{x^3}) g_{11} \dd{x^1} + \qty(\pdv{a_1}{x^3} - \pdv{a_3}{x^1}) g_{22} \dd{x^2} + \qty(\pdv{a_2}{x^1} - \pdv{a_1}{x^2}) g_{33} \dd{x^3}],
	\end{align*}
	using Levi-Civita symbol, we can write the curl of a 1-form more compactly
	\begin{equation}
		\star \dd(a_j \dd{x^j}) = \frac{1}{\sqrt{\det g}} \eps_{ijk} \pdv{a_k}{x^j} g_{ii} \dd{x^i}.
	\end{equation}
	Notice that to get the curl in a ordinary vector form instead of the covariant differential form, we need to relate the basis vector $\{ \vu{e}_i \}$ to $\{ \sqrt{g_{ii}} \dd{x^i} \}$.
	\begin{example}
		In spherical coordinates a vector can be written as
		\begin{align*}
			\bfA &= A_r \vu{e}_r + A_{\theta} \vu{e}_{\theta} + A_{\phi} \vu{e}_{\phi} \\
			&= A_r \dd{r} + r A_{\theta} \dd{\theta} + r \sin{\theta} A_{\phi} \dd{\phi}
		\end{align*}
		Therefore its curl
		\begin{align*}
			\lefteqn{\star \dd(A_r \dd{r} + r A_{\theta} \dd{\theta} + r \sin{\theta} A_{\phi} \dd{\phi})} \\
			=& \frac{1}{r^2 \sin{\theta}}  \left[\pqty{\pdv{(r \sin{\theta} A_{\phi})}{\theta} - \pdv{(r A_{\theta})}{\phi}} \dd{r} \right. \\
			&+ \pqty{\pdv{A_r}{\phi} - \pdv{(r \sin{\theta} A_{\phi})}{r}} r^2 \dd{\theta} \\
			&+ \left. \pqty{\pdv{(r A_{\theta})}{r} - \pdv{A_r}{\theta}} r^2 \sin^2{\theta} \dd{\phi} \right],
		\end{align*}
		where we can change the basis such that
		\begin{align*}
			\curl{\bfA} =& \frac{1}{r \sin{\theta}} \pqty{\pdv{ \sin{\theta} A_{\phi}}{\theta} - \pdv{ A_{\theta}}{\phi}} \vu{e}_{r} \\
			&+ \frac{1}{r \sin{\theta}} \pqty{\pdv{A_r}{\phi} - \sin{\theta} \pdv{(r A_{\phi})}{r}} \vu{e}_{\theta} \\
			&+ \frac{1}{r} \pqty{\pdv{(r A_{\theta})}{r} - \pdv{A_r}{\theta}} \vu{e}_{\phi}.
		\end{align*}
	\end{example}
	Similarly the divergence of a 1-form is given by $\star\dd{\star}$, such that
	\begin{align*}
		\star\dd{\star} (a_i \dd{x^i}) &= \star\dd \pqty{a_i \thalf \eps_{i, I} \frac{\sqrt{\det g}}{g_{ii}} \dd{x^{I}}} \\
		&= \star \bqty{\pdv{x^j} \pqty{\frac{\sqrt{\det g}}{g_{ii}} a_i} \thalf \eps_{i, I} \dd x^j \wedge \dd{x^{I}}} \\
		&= \frac{1}{\sqrt{\det g}} \bqty{\pdv{x^j} \pqty{\frac{\sqrt{\det g}}{g_{ii}} a_i} \thalf \eps_{i, I} \eps_{j, I}} \\
		&= \frac{1}{\sqrt{\det g}} \pdv{x^i} \pqty{\frac{\sqrt{\det g}}{g_{ii}} a_i},
	\end{align*}
	where we've used the property of Levi-Civita symbol $\eps_{i, I} \eps_{j, I} = 2 \delta_{ij}$.

	The Laplacian of a zero form is given by $\star\dd{\star\dd{u}}$ such that
	\begin{equation}
		\star\dd{\star\dd{u}} = \star\dd{\star} \pqty{\pdv{u}{x^i} \dd{x^i}} \\
		= \frac{1}{\sqrt{\det g}} \pdv{x^i} \pqty{\frac{\sqrt{\det g}}{g_{ii}} \pdv{u}{x^i}}.
	\end{equation}

	\subsection{Eigenvalues of $\bfJ^2$ and $J_z$}

	From the basic commutation relations of angular momentum
	\begin{equation}
		[J_k, J_l] = i \eps_{klm} J_m,
	\end{equation}
	we can obtain that $[\bfJ^2, J_l] = 0$, where we've set the Planck's constant $\hbar = 1$, which is easy to prove by the antisymmetric property of Levi-Civita symbol $\eps_{klm}$
	\begin{align*}
		[\bfJ^2, J_l] &= J_k [J_k, J_l] + [J_k, J_l] J_k \\
		&= i \eps_{klm} J_k J_m + i \eps_{klm} J_m J_k \\
		&= i (\eps_{klm} + \eps_{mlk}) J_k J_m \\
		&= 0.
	\end{align*}
	We now look at the simultaneous eigenkets of $\bfJ^2$ and $J_z$.
	\begin{equation}
		\bfJ^2 \ket{a,b} = a \ket{a,b}, \quad J_z \ket{a,b} = b \ket{a,b}.
	\end{equation}
	To determine the allowed values for $a$ and $b$, it is convenient to work with the non-Hermitian operators
	\begin{equation}
		J_{\pm} = J_x \pm i J_y,
	\end{equation}
	which are called the ladder operators. They satisfy the commutation relations
	\begin{equation}
		[J_+, J_-] = 2 J_z
	\end{equation}
	and
	\begin{equation}
		[J_z, J_{\pm}] = \pm J_{\pm}.
	\end{equation}
	This can be proved by the basic commutation relation of angular momentum operators. We can compute $J_+ J_-$ and $J_- J_+$ respectively
	\begin{align*}
		J_{+} J_{-} &= (J_x + i J_y) (J_x - i J_y) \\
		&= J_x^2 + J_y^2  + J_z \\
		&= \bfJ^2 - J_z^2 + J_z,
	\end{align*}
	and
	\begin{align*}
		J_{-} J_{+} &= (J_x - i J_y) (J_x + i J_y) \\
		&= J_x^2 + J_y^2  - J_z \\
		&= \bfJ^2 - J_z^2 - J_z.
	\end{align*}
	What is the physical meaning of $J_{\pm}$? To answer this, we examine how $J_z$ acts on $J_{\pm} \ket{a,b}$:
	\begin{align*}
		J_z J_{\pm} \ket{a,b} &= ([J_z, J_{\pm}] + J_{\pm} J_z) \ket{a,b} \\
		&= (b \pm 1) J_{\pm} \ket{a,b},
	\end{align*}
	which means $J_{\pm} \ket{a,b}$ is also an eigenket of $J_z$ with an eigenvalue $(b \pm 1)$. Note also that
	\begin{equation}
		[\bfJ^2, J_{\pm}] = 0.
	\end{equation}
	Suppose we apply $J_+$ successively, say $n$ times, to a simultaneous eigenket of $\bfJ^2$ and $J_z$. We then obtain another eigenket of $\bfJ^2$ and $J_z$ with the $J_z$ eigenvalue increased by $n$, while its $\bfJ^2$ eigenvalue is unchanged. However, this process cannot go on indefinitely. It turns out that there exists an upper limit to $b$ (the $J_z$ eigenvalue) for a given $a$ (the $\bfJ^2$ eigenvalue):
	\begin{equation}
		a \ge b^2. \label{eq: upper limit to b}
	\end{equation}
	To prove this we first note that
	\begin{align*}
		\bfJ^2 - J_z^2 &= \thalf (J_+ J_- + J_- + J_+) \\
		&= \thalf (J_+ J_+^{\dagger} + J_+^{\dagger} J_+),
	\end{align*}
	which is positive-semidefinite, thus
	\begin{equation}
		a - b^2 = \ev{(\bfJ^2 - J_z^2)}{a,b} \ge 0.
	\end{equation}
	It therefore follows that there must be a $b_{\max}$ such that
	\begin{equation}
		J_+ \ket{a, b_{\max}} = 0. \label{eq: bmax}
	\end{equation}
	Stated another way, the eigenvalue of $J_z$ cannot be increased beyond $b_{\max}$. Now \eqref{eq: bmax} also implies
	\begin{align*}
		J_- J_+ \ket{a, b_{\max}} &= (\bfJ^2 - J_z^2 - J_z) \\
		&= (a - b_{\max}^2 - b_{\max}) \ket{a, b_{\max}}\\
		&= 0.
	\end{align*}
	Because $\ket{a, b_{\max}}$ itself is not a null ket, this relationship is possible only if
	\begin{equation}
		a - b_{\max}^2 - b_{\max} = 0.
	\end{equation}
	In a similar manner, we argue from \eqref{eq: upper limit to b} that there must also exist a $b_{\min}$ such that
	\begin{equation}
		J_- \ket{a, b_{\min}} = 0.
	\end{equation}
	In analogy with \eqref{eq: bmax}, we conclude that
	\begin{equation}
		a = b_{\min}^2 - b_{\min}.
	\end{equation}
	\begin{proposition}
		Let $x \ge y$, and $x, y \in \bbR$. Show that $x^2 + x = y^2 - y$, iff $x = -y$.
	\end{proposition}
	\begin{proof}
		Sufficiency: If $x = -y$, then
		\begin{equation}
			x^2 + x = y^2 - y.
		\end{equation}
		Necessity: If $x^2 + x = y^2 - y$, then
		\begin{equation}
			x^2 - y^2 = -(x + y). \label{eq: xy trivial}
		\end{equation}
		If $x+y \neq 0$, then $x - y = -1$, which contradicts the assumption $x \ge y$. The only way to satisfy this equaiton \eqref{eq: xy trivial} is to set $x + y = 0$, i.e., $x = -y$.
	\end{proof}
	We infer that $b_{\max} = -b_{\min}$, with $b_{\max}$ positive (since $\mathbf{J}^2$ is positive-semidefinite, $a = b_{\max}^2 + b_{\max} \ge 0$). The allowed values of $b$ lie within
	\begin{equation}
		-b_{\max} \le b \le b_{\max}.
	\end{equation}

	\subsection{Angular-momentum eigenkets under rotation of $2\pi$}

	We want to show under a rotation of $2\pi$ around $y$ axis, the angular momentum eigenkets will acquire a phase factor
	\begin{equation}
		\e^{-i 2\pi J_y / \hbar}\ket{jm} = (-)^{2j}\ket{jm}.
	\end{equation}
	Recall in Chapter 3.9, Sakurai, we have
	\begin{equation}
		\mathcal{D}(\alpha = 0, \beta, \gamma = 0) \ket{jm} = \e^{-i J_y \beta / \hbar}\ket{jm} = \sum_{m'} \ket{jm'} d^{(j)}_{m'm}(\beta)
	\end{equation}
	and by the Wigner's formula for $d^{(j)}_{m'm}(\beta)$:
	\begin{align*}
		d^{(j)}_{m'm}(\beta) =& \sum_{k}(-)^{k-m+m'} \frac{\sqrt{(j+m)!(j-m)!(j+m')!(j-m')!}}{(j+m-k)!k!(j-k-m')!(k-m+m')!} \\
		& \times (\cos{\frac{\beta}{2}})^{2j-2k+m-m'} (\sin{\frac{\beta}{2}})^{2k-m+m'}.
	\end{align*}
	If $\beta = 2\pi$, the only term contributes is $k=0, m'=m$, such that
	\begin{equation}
		d^{(j)}_{mm}(2 \pi) = (-)^{2j}.
	\end{equation}
	This completes our proof.

	\subsection{Proof of the equation below (6.4.3) in Sakurai}
	\begin{proof}
		We know from (6.4.3) that $L_z \ket{k\hat{z}} = 0$, so we have
		\begin{equation}
			\mel{E'l'm'}{L_z}{k \hat{z}} = \hbar m' \braket{E'l'm'}{k\hat{z}} = 0.
		\end{equation}
		Hence if $m' \neq 0$, then $\braket{E'l'm'}{k\hat{z}} = 0$.
	\end{proof}

	\subsubsection{Figure 6.1 in Sakurai}
	We want to calculate the positions of poles are those in figure 6.1 in Sakurai. Poles of the integrand corresponds to the root of the following equation
	\begin{equation}
		k'^2 = k^2 \pm i \epsilon.
	\end{equation}
	we look at the plus sign first.
	\begin{align*}
		k'^2 &= k^2 + i \epsilon, \\
		k' &= (k^2 + i \epsilon)^{1/2} \\
		&= \qty(\sqrt{k^4 + \epsilon^2} \exp(i\phi + 2n \pi))^{1/2} \\
		&= k \qty(1+ \qty(\frac{\epsilon}{k^2})^2)^{1/4} \e^{i \phi /2} (-)^n \\
		&= \pm \qty[k+ \mathcal{O}(\epsilon^2)] (\cos(\phi/2) + i \sin(\phi/2)) \\
		&= \pm \qty[k+ \mathcal{O}(\epsilon^2)] (1 + \mathcal{O}(\epsilon^2) + i \phi/2) \\
		&= \pm (k + i \epsilon + \mathcal{O}(\epsilon^2)),
	\end{align*}
	where we use $\tan(\phi) = \epsilon/k^2 \to 0$, so $\tan(\phi) \sim \phi \to 0$.

	\subsection{s-wave scattering and zero energy bound states}
	We look at a rectangular potential which is attractive
	\begin{equation}
		V(r) =
		\begin{cases}
				-V_0,& r<r_0 \\
				0,& r>r_0
		\end{cases}.
	\end{equation}
	where $V_0 >0$. For low energy, we focus on s-wave scattering. Inside the potential the wave function $u(r) = r A_0(r) \to 0$, as $r\to 0$, because $A_0(r)$ must be finite at $r = 0$. Hence
	\begin{equation}
		u(r) = A \sin(k'r),
	\end{equation}
	where $k' = \sqrt{2m(E+V_0)/\hbar^2}$ is the wave number inside the potential. Outside the potential we have a phase shift $\delta_0$, so we have
	\begin{equation}
		u(r) = B \sin(kr + \delta_0),
	\end{equation}
	where $k = \sqrt{2mE/\hbar^2}$ is the wave number outside the potential. Next we match the boundary conditions
	\begin{align*}
		A \sin(k'r_0) &= B \sin(kr_0 + \delta_0), \\
		A k' \cos(k' r_0) &= B k \cos(kr_0 + \delta_0).
	\end{align*}
	Devide the two equations above we have
	\begin{equation}
		\tan(kr_0 + \delta_0) = \frac{k}{k'} \tan(k' r_0),
	\end{equation}
	or
	\begin{equation}
		\delta_0 = -kr_0 + \tan^{-1}\qty(\frac{k}{k'}\tan(k'r_0)).
	\end{equation}
	Next let's look at the $S$-matrix
	\begin{align*}
		S_0 &= \e^{2i\delta_0} = \e^{-2ikr_0} \e^{2i(kr_0 + \delta_0)} \\
		&= \e^{-2ikr_0} \frac{1+ i \tan(kr_0 + \delta_0)}{1 - i \tan(k r_0 + \delta_0)} \\
		&= \e^{-2ikr_0} \frac{1 + i \frac{k}{k'} \tan(k' r_0)}{1 - i \frac{k}{k'} \tan(k' r_0)}.
	\end{align*}
	$S_0$ have poles if
	\begin{equation}
		1 - i \frac{k}{k'} \tan(k' r_0) = 0, \label{eq: 1.29.1}
	\end{equation}
	i.e.
	\begin{equation}
		k = -i k' \cot(k' r_0).
	\end{equation}
	For low energy scattering, $E \to 0$, $k\to 0$, so $k' \to \sqrt{2mV_0/\hbar^2} \neq 0$. To satisfy \eqref{eq: 1.29.1}, we must have
	\begin{equation}
		\cot(k' r_0) \to 0,
	\end{equation}
	i.e.
	\begin{equation}
		k' r_0 = \sqrt{\frac{2mV_0}{\hbar^2}} r_0 \to \qty(n + \frac{1}{2}) \pi.
	\end{equation}
	This corresponds to the divergence of scattering length $a \to - \infty$ where
	\begin{align*}
		a &\equiv -\lim_{k \to 0^+} \dv{\delta_0}{k} \\
		&= r_0 - \lim_{k \to 0^+} \frac{\frac{1}{k'}\tan(k'r_0)}{1 + \qty(\frac{k}{k'}\tan(k'r_0))^2} \\
		&= r_0 - \lim_{k \to 0^+} \frac{\tan(k'r_0)}{k'} \\
		&= r_0 - \frac{\tan(\sqrt{\frac{2mV_0}{\hbar^2}} r_0)}{\sqrt{\frac{2mV_0}{\hbar^2}} r_0}.
	\end{align*}
	If $k' r_0 \to (n + 1/2) \pi$, then $\abs{\tan(k'r_0)} \to \infty$, so we have $\abs{a} \to \infty$. This means scattering length diverges whenever pole in $S_0$ as $k \to 0$.

	Next we will see that bound states with energy $E < 0$ also corresponds to poles of $S_0$. Outside the potential
	\begin{equation}
		u = B \e^{- \kappa r}\qc \kappa = ik = \sqrt{\frac{2m(-E)}{\hbar^2}},
	\end{equation}
	Notice that the energy
	\begin{equation}
		E = \frac{\hbar^2 k^2}{2m} = - \frac{\hbar^2 \kappa^2}{2m} = - \frac{\hbar^2 \abs{k}^2}{2m} < 0.
	\end{equation}
	Inside the potential
	\begin{equation}
		u = A \sin(k' r)\qc k' = \sqrt{\frac{2m(E + V_0)}{\hbar^2}}.
	\end{equation}
	Match the boundary conditions
	\begin{align*}
		A \sin(k' r_0) &= B \e^{- \kappa r_0}, \\
		A k' \cos(k' r_0) &= - B \kappa \e^{- \kappa r_0}.
	\end{align*}
	Divide the above equations we have
	\begin{equation}
		\kappa = k' \cot(k' r_0) \Longrightarrow k = -i k' \cot(k' r_0).
	\end{equation}
	

\end{document}
